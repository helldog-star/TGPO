import argparse
import os

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer


def unify_vocab_to_tokenizer_max(
    student_model_path: str,
    teacher_model_path: str,
    output_dir: str = None,
    unify_special_tokens: bool = True,
    # add_think_to_chat_template: bool = False,
):
    """
    ç»Ÿä¸€ Student å’Œ Teacher çš„è¯è¡¨å¤§å°åˆ° tokenizer çš„æœ€å¤§å€¼ï¼Œå¹¶ç»Ÿä¸€ç‰¹æ®Š token
    
    è¿™æ˜¯æœ€å®‰å…¨çš„æ–¹æ¡ˆï¼Œå› ä¸ºï¼š
    1. tokenizer å†³å®šäº†å®é™…èƒ½ç”Ÿæˆçš„ token
    2. æ¨¡å‹çš„ embedding å±‚åº”è¯¥èƒ½å®¹çº³ tokenizer ç”Ÿæˆçš„æ‰€æœ‰ token
    3. é¿å… tokenizer ç”Ÿæˆçš„ token è¶…å‡ºæ¨¡å‹èŒƒå›´
    4. ç»Ÿä¸€ç‰¹æ®Š token ç¡®ä¿ RLHF è®­ç»ƒçš„ä¸€è‡´æ€§
    
    Args:
        student_model_path: Student æ¨¡å‹è·¯å¾„
        teacher_model_path: Teacher æ¨¡å‹è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸º Noneï¼Œåˆ™åœ¨åŸè·¯å¾„åæ·»åŠ  -alignedï¼‰
        unify_special_tokens: æ˜¯å¦ç»Ÿä¸€ç‰¹æ®Š tokenï¼ˆEOS, PAD, BOS, UNKï¼‰
    """
    
    print("=" * 80)
    print("æ¨¡å‹å¯¹é½å·¥å…· (è¯è¡¨ + ç‰¹æ®Š Token)")
    print("=" * 80)
    
    # ========== ç¬¬ä¸€æ­¥ï¼šåŠ è½½ tokenizer ==========
    print("\nğŸ“– åŠ è½½ tokenizer...")
    student_tokenizer = AutoTokenizer.from_pretrained(student_model_path)
    teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_path)
    
    student_tokenizer_vocab = len(student_tokenizer)
    teacher_tokenizer_vocab = len(teacher_tokenizer)
    
    print(f"  Student tokenizer vocab: {student_tokenizer_vocab}")
    print(f"  Teacher tokenizer vocab: {teacher_tokenizer_vocab}")
    
    # ========== ç¬¬äºŒæ­¥ï¼šç¡®å®šç›®æ ‡è¯è¡¨å¤§å° ==========
    target_vocab_size = max(student_tokenizer_vocab, teacher_tokenizer_vocab)
    print(f"\nğŸ¯ ç›®æ ‡è¯è¡¨å¤§å°: {target_vocab_size}")
    print(f"   (åŸºäº tokenizer çš„æœ€å¤§å€¼)")
    
    # ========== ç¬¬ä¸‰æ­¥ï¼šåŠ è½½æ¨¡å‹ ==========
    print("\nğŸ”„ åŠ è½½æ¨¡å‹...")
    student_model = AutoModelForCausalLM.from_pretrained(
        student_model_path,
        torch_dtype=torch.bfloat16,
        device_map="cpu"
    )
    teacher_model = AutoModelForCausalLM.from_pretrained(
        teacher_model_path,
        torch_dtype=torch.bfloat16,
        device_map="cpu"
    )
    
    print(f"  Student embedding å±‚: {student_model.get_input_embeddings().weight.shape[0]}")
    print(f"  Teacher embedding å±‚: {teacher_model.get_input_embeddings().weight.shape[0]}")
    
    # ========== ç¬¬å››æ­¥ï¼šè°ƒæ•´è¯è¡¨å¤§å° ==========
    print(f"\nğŸ”§ è°ƒæ•´æ¨¡å‹è¯è¡¨å¤§å°åˆ° {target_vocab_size}...")
    
    if student_model.config.vocab_size != target_vocab_size:
        print(f"  è°ƒæ•´ Student: {student_model.config.vocab_size} -> {target_vocab_size}")
        student_model.resize_token_embeddings(target_vocab_size)
    else:
        print(f"  Student å·²åŒ¹é…")
    
    if teacher_model.config.vocab_size != target_vocab_size:
        print(f"  è°ƒæ•´ Teacher: {teacher_model.config.vocab_size} -> {target_vocab_size}")
        teacher_model.resize_token_embeddings(target_vocab_size)
    else:
        print(f"  Teacher å·²åŒ¹é…")
    
    # ========== ç¬¬äº”æ­¥ï¼šç»Ÿä¸€ç‰¹æ®Š Token ==========
    if unify_special_tokens:
        print(f"\nğŸ”§ ç»Ÿä¸€ç‰¹æ®Š Token...")
        _unify_special_tokens(
            student_tokenizer, student_model,
            teacher_tokenizer, teacher_model, False
        )
    
    # ========== ç¬¬å…­æ­¥ï¼šéªŒè¯ ==========
    print(f"\nâœ… éªŒè¯è°ƒæ•´ç»“æœ...")
    print(f"  Student:")
    print(f"    - config.vocab_size: {student_model.config.vocab_size}")
    print(f"    - embedding å±‚: {student_model.get_input_embeddings().weight.shape[0]}")
    print(f"    - tokenizer: {len(student_tokenizer)}")
    print(f"    - eos_token_id: {student_tokenizer.eos_token_id}")
    print(f"    - pad_token_id: {student_tokenizer.pad_token_id}")
    
    print(f"  Teacher:")
    print(f"    - config.vocab_size: {teacher_model.config.vocab_size}")
    print(f"    - embedding å±‚: {teacher_model.get_input_embeddings().weight.shape[0]}")
    print(f"    - tokenizer: {len(teacher_tokenizer)}")
    print(f"    - eos_token_id: {teacher_tokenizer.eos_token_id}")
    print(f"    - pad_token_id: {teacher_tokenizer.pad_token_id}")
    
    # éªŒè¯è¯è¡¨å¤§å°
    assert student_model.config.vocab_size == target_vocab_size, \
        f"Student vocab_size ä¸åŒ¹é…: {student_model.config.vocab_size} != {target_vocab_size}"
    assert teacher_model.config.vocab_size == target_vocab_size, \
        f"Teacher vocab_size ä¸åŒ¹é…: {teacher_model.config.vocab_size} != {target_vocab_size}"
    
    # éªŒè¯ç‰¹æ®Š token
    if unify_special_tokens:
        assert student_tokenizer.eos_token_id == teacher_tokenizer.eos_token_id, \
            f"EOS token ä¸åŒ¹é…: {student_tokenizer.eos_token_id} != {teacher_tokenizer.eos_token_id}"
        assert student_tokenizer.pad_token_id == teacher_tokenizer.pad_token_id, \
            f"PAD token ä¸åŒ¹é…: {student_tokenizer.pad_token_id} != {teacher_tokenizer.pad_token_id}"
    
    # ========== ç¬¬ä¸ƒæ­¥ï¼šä¿å­˜ ==========
    if output_dir is None:
        output_student = student_model_path + "-aligned"
        output_teacher = teacher_model_path + "-aligned"
    else:
        output_student = os.path.join(output_dir, "student-aligned")
        output_teacher = os.path.join(output_dir, "teacher-aligned")

    
    print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
    os.makedirs(output_student, exist_ok=True)
    os.makedirs(output_teacher, exist_ok=True)
    
    student_model.save_pretrained(output_student)
    student_tokenizer.save_pretrained(output_student)
    
    teacher_model.save_pretrained(output_teacher)
    teacher_tokenizer.save_pretrained(output_teacher)
    
    print(f"  âœ… Student å·²ä¿å­˜åˆ°: {output_student}")
    print(f"  âœ… Teacher å·²ä¿å­˜åˆ°: {output_teacher}")
    
    # ========== ç¬¬å…«æ­¥ï¼šç”Ÿæˆè¯Šæ–­æŠ¥å‘Š ==========
    print(f"\nğŸ“Š æœ€ç»ˆå¯¹é½çŠ¶æ€æŠ¥å‘Š...")
    print(f"  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print(f"  â”‚             â”‚ config   â”‚ embeddingâ”‚ tokenizerâ”‚ eos_id   â”‚")
    print(f"  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(f"  â”‚ Student     â”‚ {student_model.config.vocab_size:8d} â”‚ {student_model.get_input_embeddings().weight.shape[0]:8d} â”‚ {len(student_tokenizer):8d} â”‚ {student_tokenizer.eos_token_id:8d} â”‚")
    print(f"  â”‚ Teacher     â”‚ {teacher_model.config.vocab_size:8d} â”‚ {teacher_model.get_input_embeddings().weight.shape[0]:8d} â”‚ {len(teacher_tokenizer):8d} â”‚ {teacher_tokenizer.eos_token_id:8d} â”‚")
    print(f"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    print("\n" + "=" * 80)
    print("âœ… æ¨¡å‹å¯¹é½å®Œæˆï¼")
    print("=" * 80)
    
    return output_student, output_teacher


def _unify_special_tokens(
    student_tokenizer,
    student_model,
    teacher_tokenizer,
    teacher_model,
    use_teacher_tokens: bool = True
):
    """
    ç»Ÿä¸€ç‰¹æ®Š tokenï¼ˆEOS, PAD, BOS, UNKï¼‰
    
    Args:
        use_teacher_tokens: å¦‚æœ Trueï¼Œä½¿ç”¨ Teacher çš„ç‰¹æ®Š tokenï¼›å¦åˆ™ä½¿ç”¨ Student çš„
    """
    
    # å®šä¹‰è¦ç»Ÿä¸€çš„ç‰¹æ®Š token
    special_tokens_to_unify = [
        'eos_token_id',
        'pad_token_id',
        'bos_token_id',
        'unk_token_id',
    ]
    
    print("\n  ğŸ“‹ å½“å‰ç‰¹æ®Š Token:")
    print(f"    {'Token':15s} {'Student':10s} {'Teacher':10s} {'Status':10s}")
    print(f"    {'-' * 50}")
    
    # æ˜¾ç¤ºå½“å‰çŠ¶æ€
    for token_name in special_tokens_to_unify:
        student_val = getattr(student_tokenizer, token_name, None)
        teacher_val = getattr(teacher_tokenizer, token_name, None)
        
        if student_val == teacher_val:
            status = "âœ… åŒ¹é…"
        else:
            status = "âŒ ä¸åŒ¹é…"
        
        print(f"    {token_name:15s} {str(student_val):10s} {str(teacher_val):10s} {status:10s}")
    
    # ç¡®å®šç›®æ ‡ token
    if use_teacher_tokens:
        print(f"\n  ğŸ¯ ä½¿ç”¨ Teacher çš„ç‰¹æ®Š Token")
        target_tokens = {
            token_name: getattr(teacher_tokenizer, token_name, None)
            for token_name in special_tokens_to_unify
        }
    else:
        print(f"\n  ğŸ¯ ä½¿ç”¨ Student çš„ç‰¹æ®Š Token")
        target_tokens = {
            token_name: getattr(student_tokenizer, token_name, None)
            for token_name in special_tokens_to_unify
        }
    
    # æ›´æ–° Student
    print(f"\n  ğŸ”§ æ›´æ–° Student ç‰¹æ®Š Token:")
    for token_name, target_val in target_tokens.items():
        current_val = getattr(student_tokenizer, token_name, None)
        if current_val != target_val and target_val is not None:
            print(f"    {token_name}: {current_val} -> {target_val}")
            setattr(student_tokenizer, token_name, target_val)
            setattr(student_model.config, token_name, target_val)
    
    # æ›´æ–° Teacher
    print(f"\n  ğŸ”§ æ›´æ–° Teacher ç‰¹æ®Š Token:")
    for token_name, target_val in target_tokens.items():
        current_val = getattr(teacher_tokenizer, token_name, None)
        if current_val != target_val and target_val is not None:
            print(f"    {token_name}: {current_val} -> {target_val}")
            setattr(teacher_tokenizer, token_name, target_val)
            setattr(teacher_model.config, token_name, target_val)
    
    print(f"\n  âœ… ç‰¹æ®Š Token ç»Ÿä¸€å®Œæˆ")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å¯¹é½ Student ä¸ Teacher çš„ tokenizerï¼ˆè¯è¡¨ã€ç‰¹æ®Š tokenï¼‰")
    parser.add_argument("--student", required=True, help="Student æ¨¡å‹è·¯å¾„")
    parser.add_argument("--teacher", required=True, help="Teacher æ¨¡å‹è·¯å¾„ï¼ˆä½œä¸ºè¯è¡¨ä¸ç‰¹æ®Š token çš„å‚è€ƒï¼‰")
    parser.add_argument("--output-dir", default=None, help="è¾“å‡ºæ ¹ç›®å½•ï¼›é»˜è®¤åœ¨å„è‡ªè·¯å¾„ååŠ  -aligned")
    parser.add_argument("--no-unify-special-tokens", action="store_true", help="ä¸ç»Ÿä¸€ç‰¹æ®Š token")
    args = parser.parse_args()

    unify_vocab_to_tokenizer_max(
        student_model_path=args.student,
        teacher_model_path=args.teacher,
        output_dir=args.output_dir,
        unify_special_tokens=not args.no_unify_special_tokens,
    )