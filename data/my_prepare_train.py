import os
import time
import json
import argparse
import pandas as pd
from datasets import load_from_disk
from transformers import AutoTokenizer
from concurrent.futures import ThreadPoolExecutor
from openai import OpenAI, APIError

# ä»¥ä¸‹ç”± parse_args() åŠ __main__ èµ‹å€¼ï¼Œä¾›åç»­å‡½æ•°ä½¿ç”¨
DATASET_PATH = None
MODEL_LOCAL_PATH = None
OUTPUT_PARQUET_PATH = None
TEMP_CACHE_JSON_PATH = None
VLLM_BASE_URL = None
MAX_WORKERS = None
BATCH_SIZE = None
tokenizer = None
client = None
SERVING_MODEL_NAME = None


def parse_args():
    parser = argparse.ArgumentParser(description="OpenR1-Math æ•°æ®å‡†å¤‡ï¼šç» vLLM æ¨ç†åç”Ÿæˆè®­ç»ƒç”¨ Parquet")
    parser.add_argument("--dataset-path", required=True, help="load_from_disk ç”¨çš„æ•°æ®é›†ç›®å½•è·¯å¾„")
    parser.add_argument("--model-local-path", required=True, help="æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œç”¨äºåŠ è½½ Tokenizer")
    parser.add_argument("--output-parquet-path", required=True, help="æœ€ç»ˆè¾“å‡º Parquet æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--temp-cache-json-path", required=True, help="æ–­ç‚¹ç»­ä¼ ç¼“å­˜ JSON è·¯å¾„")
    parser.add_argument("--vllm-base-url", default="http://localhost:8000/v1", help="vLLM OpenAI å…¼å®¹æ¥å£åœ°å€")
    parser.add_argument("--max-workers", type=int, default=32, help="æ¨ç†çº¿ç¨‹æ•°")
    parser.add_argument("--batch-size", type=int, default=16, help="æ¯æ‰¹æ¨ç†æ ·æœ¬æ•°")
    return parser.parse_args()

# --- æ–­ç‚¹ç»­ä¼ å’Œç¼“å­˜é€»è¾‘ ---
def load_cache():
    """å°è¯•åŠ è½½å·²å®Œæˆçš„ç¼“å­˜ç»“æœ"""
    if os.path.exists(TEMP_CACHE_JSON_PATH):
        print(f"âœ… å‘ç°ç¼“å­˜æ–‡ä»¶: {TEMP_CACHE_JSON_PATH}ï¼Œå°è¯•ä»æ–­ç‚¹ç»§ç»­...")
        try:
            with open(TEMP_CACHE_JSON_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
        except json.JSONDecodeError:
            print("âŒ ç¼“å­˜æ–‡ä»¶æŸåï¼Œå°†ä»å¤´å¼€å§‹ã€‚")
            return {}
    return {}

def save_cache(cache_data):
    """ä¿å­˜å½“å‰çš„ç¼“å­˜ç»“æœ"""
    with open(TEMP_CACHE_JSON_PATH, 'w', encoding='utf-8') as f:
        json.dump(cache_data, f, ensure_ascii=False, indent=2)

# --- VLLM æ¨ç†å‡½æ•° (ä½¿ç”¨ OpenAI SDK) ---
def call_vllm_api(batch_prompts, indices):
    """
    ä½¿ç”¨ OpenAI SDK è°ƒç”¨ vLLM API è¿›è¡Œæ‰¹é‡æ¨ç†
    """
    # 1. æ„é€  prompt æ–‡æœ¬ (Chat Template -> Text)
    # å› ä¸ºæˆ‘ä»¬è¦æ‰¹é‡å‘é€ï¼Œä½¿ç”¨ completions æ¥å£æ¯” chat.completions æ›´å®¹æ˜“å¤„ç† list[str]
    prompt_texts = [
        tokenizer.apply_chat_template(p, tokenize=False, add_generation_prompt=True)
        for p in batch_prompts
    ]

    try:
        # 2. å‘é€è¯·æ±‚ (Completions API æ”¯æŒ prompt ä¸ºåˆ—è¡¨)
        response = client.completions.create(
            model=SERVING_MODEL_NAME,
            prompt=prompt_texts,
            max_tokens=8192,
            temperature=0.7,
            # å¦‚æœä½ éœ€è¦ deepseek_r1 çš„æ€è€ƒè¿‡ç¨‹ï¼Œå®ƒé€šå¸¸åŒ…å«åœ¨ç”Ÿæˆçš„æ–‡æœ¬ä¸­
        )
        
        # 3. è§£æç»“æœ
        # OpenAI SDK è¿”å›çš„ choices é¡ºåºé€šå¸¸ä¸ prompt é¡ºåºä¸€è‡´ï¼Œä½†ä¸ºäº†å®‰å…¨æˆ‘ä»¬ä¾é ç´¢å¼•
        processed_results = {}
        
        # vLLM å¯¹ batch è¯·æ±‚çš„è¿”å›é¡ºåºé€šå¸¸æ˜¯å¯¹åº”çš„ï¼Œç›´æ¥éå†å³å¯
        for i, choice in enumerate(response.choices):
            original_index = indices[i]
            generated_text = choice.text.strip()
            
            # æ ¼å¼åŒ–ä¸º target å­—æ®µ
            target_value = [{"content": generated_text, "role": "assistant"}]
            processed_results[str(original_index)] = target_value
            
        return processed_results

    except APIError as e:
        print(f"âš ï¸ API æŠ¥é”™ (ç´¢å¼• {indices[0]}-{indices[-1]}): {e}")
        return {}
    except Exception as e:
        print(f"âš ï¸ æœªçŸ¥é”™è¯¯ (ç´¢å¼• {indices[0]}-{indices[-1]}): {e}")
        return {}

# --- ä¸»æ‰§è¡Œé€»è¾‘ ---
def main():
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    d = os.path.dirname(OUTPUT_PARQUET_PATH)
    if d:
        os.makedirs(d, exist_ok=True)
    d = os.path.dirname(TEMP_CACHE_JSON_PATH)
    if d:
        os.makedirs(d, exist_ok=True)

    # 1. åŠ è½½æ•°æ®é›†
    print(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {DATASET_PATH}")
    ds_full = load_from_disk(DATASET_PATH)
    # # Debug
    # ds_full = ds_full.select(range(10))
    
    # 2. åŠ è½½ç¼“å­˜å’Œç¡®å®šå¾…å¤„ç†ç´¢å¼•
    results_cache = load_cache()
    
    total_samples = len(ds_full)
    all_indices = list(range(total_samples))
    
    # æ’é™¤å·²å®Œæˆçš„ç´¢å¼•
    completed_indices = set(map(int, results_cache.keys()))
    pending_indices = [i for i in all_indices if i not in completed_indices]
    
    print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"å·²å®Œæˆæ ·æœ¬æ•°: {len(completed_indices)}")
    print(f"å¾…å¤„ç†æ ·æœ¬æ•°: {len(pending_indices)}")

    if not pending_indices:
        print("æ‰€æœ‰æ ·æœ¬å‡å·²å®Œæˆï¼Œè·³è¿‡æ¨ç†æ­¥éª¤ã€‚")
        finalize_results(ds_full, results_cache, total_samples)
        return

    # 3. æ„é€ æ‰¹é‡ä»»åŠ¡
    tasks = []
    for i in range(0, len(pending_indices), BATCH_SIZE):
        batch_indices = pending_indices[i : i + BATCH_SIZE]
        batch_prompts = [ds_full[j]["prompt"] for j in batch_indices]
        tasks.append((batch_prompts, batch_indices))
        
    print(f"å‡†å¤‡æ‰§è¡Œ {len(tasks)} ä¸ªæ‰¹æ¬¡ä»»åŠ¡ï¼Œçº¿ç¨‹æ•°: {MAX_WORKERS}")

    # 4. å¤šçº¿ç¨‹æ‰§è¡Œä»»åŠ¡
    futures = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        for batch_prompts, indices in tasks:
            future = executor.submit(call_vllm_api, batch_prompts, indices)
            futures.append(future)

        # 5. å®æ—¶å¤„ç†ç»“æœå’Œæ–­ç‚¹ä¿å­˜
        for i, future in enumerate(futures):
            try:
                batch_results = future.result()
                results_cache.update(batch_results)
                
                # æ¯å¤„ç† 50 ä¸ªæ‰¹æ¬¡æˆ–ä»»åŠ¡ç»“æŸæ—¶ä¿å­˜ä¸€æ¬¡ç¼“å­˜
                if (i + 1) % 50 == 0 or i == len(futures) - 1:
                    save_cache(results_cache)
                    elapsed = time.time() - start_time
                    progress = len(results_cache) / total_samples
                    speed = len(results_cache) / elapsed if elapsed > 0 else 0
                    print(
                        f"ğŸ”¥ Progress: {len(results_cache)}/{total_samples} ({progress:.2%}) "
                        f"| Elapsed: {elapsed:.2f}s | Speed: {speed:.2f} samples/s"
                    )

            except Exception as e:
                print(f"ğŸ”¥ æ•è·åˆ°çº¿ç¨‹æ‰§è¡Œé”™è¯¯: {e}")
                
    # 6. æœ€ç»ˆä¿å­˜
    finalize_results(ds_full, results_cache, total_samples)


def finalize_results(ds_full, results_cache, total_samples):
    """å°†ç¼“å­˜ç»“æœåˆå¹¶åˆ°Datasetå¹¶ä¿å­˜ä¸ºParquet"""
    
    print("\n--- ä»»åŠ¡å®Œæˆï¼Œå¼€å§‹æœ€ç»ˆæ•°æ®åˆå¹¶ ---")
    
    # å°†ç¼“å­˜çš„å­—å…¸è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œç¡®ä¿é¡ºåºå’Œå®Œæ•´æ€§
    all_targets = [None] * total_samples
    valid_count = 0
    for i in range(total_samples):
        key = str(i)
        if key in results_cache:
            all_targets[i] = results_cache[key]
            valid_count += 1
        else:
            # å¦‚æœå­˜åœ¨æœªå®Œæˆçš„æ ·æœ¬ï¼Œä½¿ç”¨åŸå§‹çš„targetå­—æ®µæˆ–è®¾ç½®ä¸ºç©º
            all_targets[i] = ds_full[i]["target"] if "target" in ds_full[i] else [{"content": "ERROR_OR_PENDING", "role": "assistant"}]
            
    print(f"æœ€ç»ˆæœ‰æ•ˆç»“æœæ•°: {valid_count}/{total_samples}")
    
    # å°† target åˆ—è¡¨æ·»åŠ åˆ°åŸå§‹æ•°æ®é›†çš„å‰¯æœ¬ä¸­
    final_ds = ds_full.add_column("new_target", all_targets)
    final_ds = final_ds.remove_columns(["target"]).rename_column("new_target", "target")
    
    # ä¿å­˜ä¸º Parquet
    final_ds.to_parquet(OUTPUT_PARQUET_PATH)
    print(f"ğŸ‰ æœ€ç»ˆç»“æœå·²ä¿å­˜åˆ°: {OUTPUT_PARQUET_PATH}")
    
    # æ¸…ç†ç¼“å­˜æ–‡ä»¶
    if os.path.exists(TEMP_CACHE_JSON_PATH):
        os.remove(TEMP_CACHE_JSON_PATH)
        print("ğŸ—‘ï¸ ä¸´æ—¶ç¼“å­˜æ–‡ä»¶å·²æ¸…é™¤ã€‚")


if __name__ == "__main__":
    args = parse_args()
    DATASET_PATH = args.dataset_path
    MODEL_LOCAL_PATH = args.model_local_path
    OUTPUT_PARQUET_PATH = args.output_parquet_path
    TEMP_CACHE_JSON_PATH = args.temp_cache_json_path
    VLLM_BASE_URL = args.vllm_base_url
    MAX_WORKERS = args.max_workers
    BATCH_SIZE = args.batch_size

    print("æ­£åœ¨åŠ è½½ Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_LOCAL_PATH, padding_side="left")
    client = OpenAI(base_url=VLLM_BASE_URL, api_key="EMPTY")
    try:
        models_list = client.models.list()
        SERVING_MODEL_NAME = models_list.data[0].id
        print(f"âœ… è¿æ¥æˆåŠŸï¼ŒæœåŠ¡ç«¯æ¨¡å‹åç§°: {SERVING_MODEL_NAME}")
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ° vLLM æœåŠ¡ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯åŠ¨: {e}")
        exit(1)

    start_time = time.time()
    main()